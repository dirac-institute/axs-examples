{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as sparkfunc\n",
    "import astropy.io.fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "matplotlib.rcParams['figure.figsize'] = [8, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def healpix_hist(input_df, NSIDE=64, groupby=[],\n",
    "                 agg={\"*\": \"count\"}, returnDf=False):\n",
    "    from pyspark.sql.functions import floor as FLOOR, col as COL, lit, shiftRight\n",
    "\n",
    "    order0 = 12\n",
    "    order  = hp.nside2order(NSIDE)\n",
    "    shr    = 2*(order0 - order)\n",
    "\n",
    "    # construct query\n",
    "    df = input_df.withColumn('hpix__', shiftRight('hpix12', shr))\n",
    "    gbcols = ('hpix__', )\n",
    "    for axspec in groupby:\n",
    "        if not isinstance(axspec, str):\n",
    "            (col, c0, c1, dc) = axspec\n",
    "            df = ( df\n",
    "                .where((lit(c0) < COL(col)) & (COL(col) < lit(c1)))\n",
    "                .withColumn(col + '_bin__', FLOOR((COL(col) - lit(c0)) / lit(dc)) * lit(dc) + lit(c0) )\n",
    "                 )\n",
    "            gbcols += ( col + '_bin__', )\n",
    "        else:\n",
    "            gbcols += ( axspec, )\n",
    "    df = df.groupBy(*gbcols)\n",
    "\n",
    "    # execute aggregation\n",
    "    df = df.agg(agg)\n",
    "\n",
    "    # fetch result\n",
    "    df = df.toPandas()\n",
    "    if returnDf:\n",
    "        return df\n",
    "\n",
    "    # repack the result into maps\n",
    "    # This results line is slightly dangerous, because some aggregate functions are purely aliases.\n",
    "    # E.g., mean(x) gets returned as a column avg(x).\n",
    "    results = [ f\"{v}({k})\" if k != \"*\" else f\"{v}(1)\" for k, v in agg.items() ]    # Result columns\n",
    "    def _create_map(df):\n",
    "        maps = dict()\n",
    "        for val in results:\n",
    "            map_ = np.zeros(hp.nside2npix(NSIDE))\n",
    "            # I think this line throws an error if there are no rows in the result\n",
    "            map_[df.hpix__.values] = df[val].values \n",
    "            maps[val] = [ map_ ]\n",
    "        return pd.DataFrame(data=maps)\n",
    "\n",
    "    idxcols = list(gbcols[1:])\n",
    "    if len(idxcols) == 0:\n",
    "        ret = _create_map(df)\n",
    "        assert(len(ret) == 1)\n",
    "        if not returnDf:\n",
    "            # convert to tuple, or scalar\n",
    "            ret = tuple(ret[name].values[0] for name in results)\n",
    "            if len(ret) == 1:\n",
    "                ret = ret[0]\n",
    "    else:\n",
    "        ret = df.groupby(idxcols).apply(_create_map)\n",
    "        ret.index = ret.index.droplevel(-1)\n",
    "        ret.index.rename([ name.split(\"_bin__\")[0] for name in ret.index.names ], inplace=True)\n",
    "        if \"count(1)\" in ret:\n",
    "                    ret = ret.rename(columns={'count(1)': 'count'})\n",
    "        if not returnDf:\n",
    "            if len(ret.columns) == 1:\n",
    "                ret = ret.iloc[:, 0]\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def bin_column(start, stop, bins, data):\n",
    "    bin_size = (stop - start)/bins\n",
    "    return sparkfunc.floor((data - start)/bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dirac import DataBase\n",
    "\n",
    "db = DataBase({\"spark.executor.instances\" : \"32\", \n",
    "               \"spark.sql.warehouse.dir\" : \"/home/stevenstetzler/S3_DB\"})\n",
    "\n",
    "spark = db.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_ = spark.read.load(\"s3a://axscatalog/gaiadr2\")\n",
    "gaia = gaia_.drop(\"hpix12\").withColumn(\"hpix12\",\n",
    "                        sparkfunc.floor(gaia_['source_id']/34359738368))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "map_ = healpix_hist(gaia.where(gaia['dup'] == 0), NSIDE=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(map_, title=\"All objects\", rot=(0, 0, 0), nest=True, norm='hist')\n",
    "plt.show()\n",
    "hp.mollview(map_, title=\"All objects\", rot=(180, 0, 0), nest=True, norm='hist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
